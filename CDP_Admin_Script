*********************************************
              CDP Admin tasks
*********************************************
> Create The Instance same as created in CDP Cloudera and the root of hadoop is "hdfs"
1.Name one of the instance of t2.large or t3.large as Worker
2.Connect to that instance by coping the public ip v4 of that instance
$ 1.ssh -i nick.pem ubuntu@public_ip
$ 2.hdfs dfs -mkdir /data
$ 3.sudo su
$ 4.cd
$ 5.hdfs dfs -mkdir /data
$ 6.exit
$ 7.su hdfs
$ 8.set the password ==> sudo passwd hdfs
$ 9.su hdfs
$ 10.sudo nano /etc/passwd
> for searching hdfs press ctrl + W ===> remove  /usr/sbin/nologin ====> and enter /bin/bash then save cltr + o enter cltr + x
$ to hdfs sudo ===>$ 11.sudo adduser hdfs sudo 
$ 12.su hdfs
$ 13.cd
$ 14.hdfs dfs -mkdir /data
> important===> home directory of hdfs user ===> /var/lib/hadoop-hdfs
$ 15.hdfs and enter 
$ 16.hdfs dfs
$ 17.hdfs dfs admin ===> admin command for file system
$ 18. for yarn
$ 19.hdfs dfs -ls /



# Safemode ====> what is safe mode is the read only mode or maintaince mode (hdfs)
 any time of maintance like update/upgrade/patch/configuration

$ 1.hdfs dfsadmin -safemode get
$ 2.hdfs dfsadmin -safemode enter
$ 3.nano abc add any data in it
$ 4.ls
$ 5.hdfs dfs -put abc /data 
$ 6.hdfs dfsadmin -safemode leave ===> exit from safe mode
$ 7.hdfs dfsadmin -safemode get ====> To check if safe mode is off or not

-----***** Creating snapshots ****----
 
$ 1.hdfs dfsadmin -allowSnapshot /data ===> if you want to create snapshot first allow the snapshot using command

$ 2.hdfs dfs -createSnapshot /data snap1

$ 3.hdfs lsSnapshottableDir

$ 4.hdfs dfs -ls /data/.snapshot ===> this command will give the list of dir on which snapshot given

$ 5.hdfs dfs -deleteSnapshot /data snap1 

> Go in cloudera click on hdfs service click on upper list Namenode web ui ===> which last number of ip then go in terminal and check private ip of all instance the 
on which match copy the private ip and past in search bar and then 9870 go in snapshot check the information 

$ 6.hdfs dfsadmin -disallowSnapshot /data

$ 7.hdfs lsSnapshottableDir

$ 8.exit

$ 9.hdfs dfs -mkdir /abc

$ 10.su hdfs 

$ 11.cd

$ 12.useradd testuser 

--> check snapshots on NN web UI


-----**** Change ownerships ****-----
$ 1.sudo addgroup grp1

$ 2.hdfs dfs -mkdir /abc

$ 3.hdfs dfs -ls /


$ 4.hdfs dfs -chown testuser /data/abc  ===> sir used this hdfs dfs -chown testuser /abc

$ 5.hdfs dfs -ls /

$ 6.hdfs dfs -chgrp group1 /data/abc ===> sir used this ==> hdfs dfs -chgrp grp1 /abc

$ 7.hdfs dfs -ls /

$ 8.hdfs dfs -chown test

$ 9.hdfs dfs -chown testuser:grp1  /data

$ 10.hdfs dfs -ls /

$ 11.hdfs dfs -chmod 700 /data/abc  sir used this ===> hdfs dfs g+w /abc

$ 12.hdfs dfs -ls /

$ 13.hdfs dfs -chmod 740 /abc

$ 14.hdfs dfs -ls /

$ 15.hdfs dfs -ls -R


-----**** Configuring the HDFS quota ****-----

$ 1.wget https://raw.githubusercontent.com/tbertinmahieux/MSongsDB/master/Tasks_Demos/CoverSongs/shs_dataset_test.txt

$ 2.ls -l

$ 3.hdfs dfs -ls /

$ 4.hdfs dfs -put abc /data

$ 5.hdfs dfs -put shs_dataset_test.txt /data

$ 6.hdfs dfs -ls /data

$ cd

$ 7.hdfs dfsadmin -setQuota 20 /data

$ 8.hdfs dfsadmin -setSpaceQuota 2 or ___ /data

$ 9.nano xyz ==> put any data

$ 10.hdfs dfs -put xyz /data

$ 11.hdfs dfsadmin -clrQuota /data

$ 12.hdfs dfs -sl /data

$ 13.hdfs dfsadmin -setSpace Quota then size in bites or watch video ===> for checking error

$ 14. hdfs dfs -put xyz /data


$ 16.hdfs dfs -count -q /path/to/directory ===> sir used this command ==> hdfs dfs -count -q /data

$ 17.hdfs dfs -count -q -h /data
$ 15.hdfs dfsadmin -clrSpaceQuota /data

quota  rem_quota  space_quota  rem_space_quota  dir_count  file_count  size  file
 none     inf        54975	    56897           3           8     786955 /u/ 

$ 18.hdfs   


> To stop the cloudera cluster
1.stop cluster CM UI
2.stop CM from CLI
3. from terminal using this command ===> $ sudo service cloudera-scm-server stop 
4. stop EC2 isntance from aws console


> GO in cloudera hdfs in upper line go in instances Find NameNode private ip and check it from the instance of aws console and copy the public_ip and connect in
another terminal 

  1.ssh -i nick.pem ubuntu@public_ip
  2.cd /dfs/nn
  3.sudo su
  4.cd
  5.cd /dfs/nn
  6. ls  
  7.cd /current
  8.nano VERSION
  9.cd ..
 10.ls
 11.cd current/
 12.ls
 13.cd
 14.mkdir nnbackup
 15.ls
 16.pwd
 17.cd /dfs/nn/
 18.cp -r current /root/nnbackup
 19. cd
 20.ls
 21.cd nnbackup/
 22.ls
 23.cd current/
 24.ls
 25.cd

 > Come in previous terminal 
1.cd

-----*** Namenode metadata backup ****-----

1.hdfs fsck / ==> TO check the file system health
 
2.hdfs dfsadmin -report ==> To check the report

3.hdfs dfsadmin -report > report.txt ==> To save the report to a file 

4.ls

5.nano report.txt

4.hdfs dfsadmin -safemode enter  ===> why are we entering safe mode ==> safe mode should insure that there is consistant backup

5.hdfs dfsadmin -saveNamespace ===> this command will do manuall check pointing

6.hdfs dfsadmin -safemode leave

7.hdfs fsck /
 
8.cd /dfs/nn


cp -r current/ /var/lib/hadoop-hdfs/nnbackup


----->>>> Leave safemode
hdfs dfsadmin -safemode leave


-----**** Filesystem check ****----

hdfs fsck <path>
hdfs fsck /

## Options for fsck. To be given after path
-delete					Delete corrupted files.
-files					Print out files being checked.
-files -blocks				Print out the block report
-files -blocks -locations		Print out locations for every block.
-includeSnapshots			Include snapshot data if the given path indicates a snapshottable directory or there are snapshottable 						directories under it.
-list-corruptfileblocks			Print out list of missing blocks and files they belong to.
-move					Move corrupted files to /lost+found.


----**** Setting Replication for a dataset ----*****

hdfs dfs -setrep -w 4 <file-name>


-----**** Yarn Administration ****----

## Submit inbuild hadoop jobs (Open 4 terminals and submit same job at a time)


yarn jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar pi 20 500

yarn application -list

yarn application -list -appStates ALL

##To get application ID use 

yarn application -list

yarn application -status application_1459542433815_0002

## Kill an application

yarn application -kill application_1459542433815_0002

yarn logs -applicationId application_1459542433815_0002
